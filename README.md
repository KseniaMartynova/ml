# Сравнение методов кластеризации на основе семантических эмбеддингов
Этот репозиторий представляет собой исследование для машинного обучения, посвященное сравнению различных методов кластеризации для задачи семантической группировки биологических терминов.

## Цель:
Сравнить эффективность двух принципиально разных подходов к кластеризации на основе векторных представлений (эмбеддингов) терминов Gene Ontology, полученных с помощью модели Hig2Vec.
Gene Ontology — это стандартизированная система терминов для описания функций генов. Каждый GO-термин имеет определенное семантическое значение. В этом проекте мы используем предобученную модель Hig2Vec, которая преобразует эти термины в векторные представления в многомерном пространстве, сохраняя их семантические связи.

## Задача кластеризации
Автоматически сгруппировать семантически близкие GO-термины в кластеры без предварительных меток, что может помочь в анализе биологических данных, сокращении размерности онтологий и выявлении скрытых паттернов.

## Методы кластеризации
В работе я сравниваю два подхода:
1) Иерархическая кластеризация
2) Понижение размерности + DBSCAN

## Критерии сравнения
1) Качество кластеризации: семантическая однородность кластеров
2) Интерпретируемость: визуальная четкость разделения на группы
3) Практическая применимость: получение разумного количества кластеров 
4) Устойчивость к шуму: способность методов выделять значимые группы

## Исходные данные
1) Все таблицы результатов aнализa обогащения по функциональной принадлежности (4 группы: Atx, Young, Old, CS). 604 термина.
2) Скрипт [semantic_model.py](https://github.com/KseniaMartynova/ml/blob/main/semantic_model.py) с функцией, которая предназначена для преобразования списка GO-терминов в матрицу числовых векторных представлений (эмбеддингов) с использованием предобученной модели Hig2Vec: ```get_semantic_matrix(terms, embedding_dict)```, возвращает матрицу размера t × d (t — число терминов, d — размерность эмбеддингов Hig2Vec). 
3) Модель машинного обучения, разработанная для генерации семантических векторных представлений терминов Gene Ontology - [Hig2Vec](https://github.com/JaesikKim/HiG2Vec?ysclid=mjowu2sbfw299414224) и словарь эмбеддингов (семантическиe векторные представления).
Hig2Vec учитывает: отношения предки + потомки между терминами, часть-of отношения и семантическую близость через общих предков в дереве GO.

### Ход работы 
1) Объединить с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/join.py) все файлы и получить общий - [combined_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/combined_terms.csv).
   В нем должны быть столбцы term и description из объединенных таблиц (Atx, Young, Old, CS) по строкам:
<img width="452" height="217" alt="image" src="https://github.com/user-attachments/assets/746aabee-8fac-4681-aea9-63c97b50e821" /> 

2) Собрать уникальные термины из всех групп в файл [all_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/all_terms.csv):  ```all_terms = sorted(set(combined_table["term"]))```
4) Получить эмбеддинги Hig2Vec с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/sorted_embedding.py).  Вызвать функцию: ```X = get_semantic_matrix(all_terms, embedding_dict)``` из [файла](https://github.com/KseniaMartynova/ml/blob/main/semantic_model.py) 
Получится матрица X, которая нужна для дальнейшей кластеризации.

Шаги с 1 по 3 были общими для обоих методов, далее я буду работать над иерархической клатеризацией и DBSCANом отдельно.

### Иерархическая кластеризация
1) Нужно построить матрицу семантической близости.
Матрица семантической близости используется в биологии для анализа взаимосвязей между биологическими объектами. Такая матрица отражает степень сходства между терминами на основе семантики, закодированной в онтологиях.
Я буду использовать косинусное сходство, которое измеряет косинус угла между двумя векторами:  
```from sklearn.metrics.pairwise import cosine_similarity S = cosine_similarity(X) ```
S — матрица t×t, где значения от -1 до 1
[Скрипт](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/s_matrix.py)
3)  Теперь сама [Иерархическая кластеризация](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/clustering_i.py).
Где D = 1 - S  - это расстояние = 1 - сходство, в результате получается значения расстояний от 0 (максимальное сходство) до 2 (минимальное сходство). Я использую агрегативный(average) подход - каждый термин начинает как отдельный кластер, потом на каждом шаге объединяются два ближайших кластера и процесс объединения продолжается до образования одного кластера. Расстояние между кластерами вычисляется как среднее расстояние между всеми парами элементов: d(u,v) = ∑(d(i,j)) / (|u| * |v|) для всех i в u, j в v
Метод average устойчив к выбросам и создает сбалансированные кластеры. В итоге получается матрица Z, которая соедржит историю слияний.
 4)  [Подбор оптимального количества кластеров](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/number_cluster_i.py). Нам нужно достичь разумного числа кластеров от 20 до 30 и 
семантического сходства примерно ~0.7
5)  [Итоговая таблица](https://github.com/KseniaMartynova/ml/blob/main/datasets/results/hierarchical/final_clusters_with_descriptions.csv). Получается в итоговой таблице: айди термина, номер кластера и описание термина.
#### Итоги иерархической кластеризации
Из [heatmaps](https://github.com/KseniaMartynova/ml/tree/main/datasets/results/hierarchical/heatmap) можно увидеть, что термины в кластерах достаточно близки по смыслу. 
Всего кластеров получилось 26.  Нас интересует семантическое сходство ≥ 0.7, в таблице ниже представлено среднее семантическое расстояние, чем оно больше, тем лушче связаны по смыслу термины в кластере, кластеры размером менее 2 не пердставлены в таблице.
| Кластер | Среднее сходство | Оценка           | Размер кластера |
|---------|------------------|------------------|-----------------|
|1.0      |      0.860       | Умеренно связаны | 4 |
|3.0      |      0,952       |Очень сильно      | 2 |
|4.0      |     0.759        |Нормально связаны |171|
|5.0      |     0.797        |Нормально связаны | 2 |
|7.0      |      0.620       |Нормально связаны | 84|
|9.0      |     0.776        |Нормально связаны |129|
|14.0     |  0.846           |Умеренно связаны  | 74 |
|16.0     |   0.922          |Очень сильно      | 3 |
|17.0     |    0.797         |Нормально связаны | 14 |
|18.0     |   0.835          |Умеренно связаны  | 105 |

Однако получилось слишком много кластеров с количеством терминов <=2 и несколько кластеров с непомерно большим количеством терминов. И мы можем наблюдать, что среднее сходство не особо удовлетвроительно. Это все очень нехорошо и достаточно бессмысленно для нашей задачи.Поэтому попробуем еще DBSCAN.


### Понижение размерности + DBSCAN
1) [Понижение размерности](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/dbscan/tsne.py) . - преобразование эмбеддингов Hig2Vec из 200 измерений в 2D пространство, при этом нужно сохранить семантические отношения между терминами. Я буду использовать t-SNE, который минимизирует расхождение между распределениями расстояний в высокоразмерном и низкоразмерном пространствах.
<img width="1279" height="1048" alt="tSNE_simple" src="https://github.com/user-attachments/assets/c1004cdf-831b-4170-94e3-4a668b6a8acd" />
2) [КластеризацияDBSCAN](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/dbscan/clustering.py) . DBSCAN не предполагает сферическую форму кластеров и работает по следующему принципу: сначала идет идентификация ядерных точек, при этом точка считается ядерной, если в радиусе eps находится ≥ min_samples точек; потом ядерные точки, достижимые друг через друга, объединяются в кластер; и остается еще обработать шум: неядерные точки, недостижимые из ядерных, помечаются как шум и заносится в кластер -1.
3) Теперь будем пытаться подобрать оптимальные кластеры: eps будем менять в диапазоне 0.5–2.5, чтобы добиться 15–30 кластеров, и шум не должен быть слишком большим. Я выбираю eps=1.8.
<img width="3180" height="2373" alt="dbscan_parameter_comparison" src="https://github.com/user-attachments/assets/9dc07348-0556-43fc-a297-2ec68e645f06" />
Кластеры для eps=1.8 выглядят хорошо и даже равномерно и создают обманчивое впечатление, получилось 19 кластеров, что кластеризация с помощью DBSKAN лучше для нашей задачи.

#### Итоги DBSCANA
Чтобы проверить насколько визуальные впечатления не обманчивы сделаем проверку, посчитаем среднее расстояние между всеми терминами из матрицы семантической близости S. Чем больше расстояние, тем термины в кластере менее связаны и наша кластеризация в общем то не имеет смысла в рамках данной задачи. Если расстояние маленькое, то сходство между терминами в кластере очень большое и термины в кластере очень семантически схожи. 

| Кластер | Расстояние | Оценка |
|---------|------------|--------|
|6.0      |      0.944 |Почти не связаны|
|3.0      |      0.948 |Почти не связаны|
|9.0      |     0.961  |Почти не связаны|
|0.0      |     0.946  |Почти не связаны|
|1.0      |      0.970 |Почти не связаны|
|2.0      |     0.928  |Почти не связаны|
|17.0     |  0.906     |Почти не связаны|
|7.0      |   0.970    |Почти не связаны|
|8.0      |    0.912   |Почти не связаны|
|13.0     |   0.931    |Почти не связаны|
|11.0     |    0.918   |Почти не связаны|
|16.0     |     0.932  |Почти не связаны|
|14.0     |     0.861  |Mало общего|
|4.0      |      1.003 |Почти не связаны|
|12.0     |     0.999  |Почти не связаны|
|10.0     |      0.754 |Mало общего|
|19.0     |    1.008   |Почти не связаны|
|5.0      |    0.872   |Mало общего|
|15.0     |     0.901  |Почти не связаны|
|18.0     |      0.635 |Умеренное сходство|
 
Вот такая таблица получилась у меня, можно сделать вывод, что кластеры красивые да и только, термины в них не связаны по смыслу. Для наглядности можно еще сделать [heatmaps](https://github.com/KseniaMartynova/ml/tree/main/datasets/results/dbscan/heatmaps). Они получаются очень пестрые и по сравнению с иерархической кластеризацей совсем нехорошие.

### Спектральная кластеризация
Спектральная кластеризация —  метод, основанный на спектральной теории графов, который рассматривает данные как узлы в графе. Метод использует собственные векторы матрицы сходства для проецирования данных в пространство более низкой размерности, где кластеры становятся более различимыми. Этот подход особенно эффективен, когда кластеры имеют сложные, невыпуклые формы.

1) Сначала сторится матрица сходства - граф, где термины - вершины, а ребра - семантические связи между ними.
2) Спектральное разложение - вычисляются собственные векторы матрицы Лапласиана графа.
3) Векторы проецируются в пространство меньшей размерности, где выполняется кластеризация.

У меня получилось 24 кластера. Ниже представлено распределение их в двумерном пространстве, для визуализации использовалось понижение размерности с помощью метода главных компонент(находит главные компоненты — линейные комбинации исходных переменных, упорядоченные по убыванию объясняемой дисперсии).
<img width="2186" height="1772" alt="spectral_clusters_pca_visualization" src="https://github.com/user-attachments/assets/e0f914e0-15a2-4db0-ba5f-aeb742169fc2" />


Чтобы оценить насколько подходящими получились кластеры я использовую матрицу косинусного сходства, вычисленная на основе исходных эмбеддингов Hig2Vec.

| Кластер |	Среднее косинусное сходство |	Оценка       |	Размер кластера  |
|---------|-----------------------------|--------------|------------------|
| 2       | 0.597	                      |Слабо связаны |	26               |
| 16      |	0.257	           |Практически не связаны |	23   |
|15|	0.205|	Практически не связаны|	21 |
|5|	0.176|	Практически не связаны|	23 |
|4	|0.174	|Практически не связаны|	31 |
|19|	0.173|	Практически не связаны|	19| 
|3	|0.166	|Практически не связаны	|32 |
|20	|0.163|	Практически не связаны	|19 |
|14	|0.152	|Практически не связаны|	19 |
|0	|0.150|	Практически не связаны	|33 |
|8|	0.147	|Практически не связаны|	38 |
|9|	0.109	|Практически не связаны	|22| 
|23	|0.103	|Практически не связаны	|27 |
|17|	0.090|	Практически не связаны|	25|
|1	|0.086	|Практически не связаны |33 |
|12|	0.078	|Практически не связаны	|39 |
|10|	| 0.075	| Практически не связаны|	13 |
|13|	| 0.061 |	Практически не связаны	|22 |
|22|	| 0.056	| Практически не связаны	|15|
|7	| 0.053 |	Практически не связаны	|26 |
|18|	0.046 |	Практически не связаны	|21 |
|11|	0.039	| Практически не связаны	|37 |
|21|	0.031	| Практически не связаны	|19 |
|6|	0.016	| Практически не связаны	|20 |

Ну и по таблице вывод следующий, размер кластеров получился очень достойный и количество кластеров тоже. Однако смысловая связность терминов внутри кластеров совсем плохая. 

## Итог работы
1) Самый лучший метод получился иерархический, но все равно недостаточно удовлетворительный.
2) Самое равномерное разбиение на кластеры дала спектральная кластеризация, но термины получились внутри кластеров совсем не связанные по смыслу.
3) Самые визуально красивые кластеры дал DBSCAN, но так же термины получились внутри кластеров совсем не связанные по смыслу, что подтвержают пестрые heatmaps.
4) Возможно дело все таки в данных а не во мне( 




