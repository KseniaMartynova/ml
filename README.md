# Сравнение методов кластеризации на основе семантических эмбеддингов
Этот репозиторий представляет собой практическое исследование в области машинного обучения, посвященное сравнению различных методов кластеризации на задаче семантической группировки биологических терминов.

## Цель:
Сравнить эффективность двух принципиально разных подходов к кластеризации на основе векторных представлений (эмбеддингов) терминов Gene Ontology, полученных с помощью модели Hig2Vec.
Gene Ontology — это стандартизированная система терминов для описания функций генов. Каждый GO-термин имеет определенное семантическое значение. В этом проекте мы используем предобученную модель Hig2Vec, которая преобразует эти термины в векторные представления в многомерном пространстве, сохраняя их семантические связи.

## Задача кластеризации
Автоматически сгруппировать семантически близкие GO-термины в кластеры без предварительных меток, что может помочь в анализе биологических данных, сокращении размерности онтологий и выявлении скрытых паттернов.

## Методы кластеризации
В работе я сравниваю два подхода:
1) Иерархическая кластеризация
2) Понижение размерности + DBSCAN

## Критерии сравнения
1) Качество кластеризации: семантическая однородность кластеров
2) Интерпретируемость: визуальная четкость разделения на группы
3) Практическая применимость: получение разумного количества кластеров 
4) Устойчивость к шуму: способность методов выделять значимые группы

## Исходные данные
1) Все таблицы результатов aнализa обогащения по функциональной принадлежности (4 группы: Atx, Young, Old, CS).
2) Скрипт [semantic_model.py](https://github.com/KseniaMartynova/ml/blob/main/semantic_model.py) с функцией, которая предназначена для преобразования списка GO-терминов в матрицу числовых векторных представлений (эмбеддингов) с использованием предобученной модели Hig2Vec: ```get_semantic_matrix(terms, embedding_dict)```, возвращает матрицу размера t × d (t — число терминов, d — размерность эмбеддингов Hig2Vec). 
3) Модель машинного обучения, разработанная для генерации семантических векторных представлений терминов Gene Ontology - [Hig2Vec](https://github.com/JaesikKim/HiG2Vec?ysclid=mjowu2sbfw299414224) и словарь эмбеддингов (семантическиe векторные представления).
Hig2Vec учитывает: отношения предки + потомки между терминами, часть-of отношения и семантическую близость через общих предков в дереве GO.

### Ход работы 
1) Объединить с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/join.py) все файлы и получить общий - [combined_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/combined_terms.csv).
   В нем должны быть столбцы term и description из объединенных таблиц (Atx, Young, Old, CS) по строкам:
<img width="452" height="217" alt="image" src="https://github.com/user-attachments/assets/746aabee-8fac-4681-aea9-63c97b50e821" /> 

2) Собрать уникальные термины из всех групп в файл [all_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/all_terms.csv):  ```all_terms = sorted(set(combined_table["term"]))```
4) Получить эмбеддинги Hig2Vec с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/sorted_embedding.py).  Вызвать функцию: ```X = get_semantic_matrix(all_terms, embedding_dict)``` из [файла](https://github.com/KseniaMartynova/ml/blob/main/semantic_model.py) 
Получится матрица X, которая нужна для дальнейшей кластеризации.

Шаги с 1 по 3 были общими для обоих методов, далее я буду работать над иерархической клатеризацией и DBSCANом отдельно.

### Иерархическая кластеризация
1) Нужно построить матрицу семантической близости.
Матрица семантической близости используется в биологии для анализа взаимосвязей между биологическими объектами. Такая матрица отражает степень сходства между терминами на основе семантики, закодированной в онтологиях.
Я буду использовать косинусное сходство, которое измеряет косинус угла между двумя векторами:  
```from sklearn.metrics.pairwise import cosine_similarity S = cosine_similarity(X) ```
S — матрица t×t, где значения от -1 до 1
[Скрипт](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/s_matrix.py)
3)  Теперь сама [Иерархическая кластеризация](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/clustering_i.py).
Где D = 1 - S  - это расстояние = 1 - сходство, в результате получается значения расстояний от 0 (максимальное сходство) до 2 (минимальное сходство). Я использую агрегативный(average) подход - каждый термин начинает как отдельный кластер, потом на каждом шаге объединяются два ближайших кластера и процесс объединения продолжается до образования одного кластера. Расстояние между кластерами вычисляется как среднее расстояние между всеми парами элементов: d(u,v) = ∑(d(i,j)) / (|u| * |v|) для всех i в u, j в v
Метод average устойчив к выбросам и создает сбалансированные кластеры. В итоге получается матрица Z, которая соедржит историю слияний.
 4)  [Подбор оптимального количества кластеров](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/number_cluster_i.py) Нам нужно достичь разумного числа кластеров от 20 до 30 и 
семантического сходства примерно ~0.7
5)  [Итоговая таблица](https://github.com/KseniaMartynova/ml/blob/main/datasets/results/hierarchical/final_clusters_with_descriptions.csv). Получается там айди термина, номер кластера и описание.
#### Итоги иерархической кластеризации
Из [heatmaps](https://github.com/KseniaMartynova/ml/tree/main/datasets/results/hierarchical/heatmap) можно увидеть, что термины в кластерах достаточно близки по смыслу. Однако получилось слишком много кластеров с количеством терминов <=2 и несколько кластеров с непомерно большим количеством терминов. Это все очень нехорошо и достаточно бессмысленно для нашей задачи. Поэтому попробуем еще DBSCAN.

### Понижение размерности + DBSCAN
1) Понижение размерности
2) Кластеризация DBSCAN

| Кластер | Расстояние | Оценка |
|---------|------------|--------|
|6.0      |      0.944 |       Почти не связаны|
|3.0      |      0.948 |Почти не связаны|
|9.0      |     0.961  |Почти не связаны|
|0.0      |     0.946  |Почти не связаны|
|1.0      |      0.970 |Почти не связаны|
|2.0      |     0.928  |Почти не связаны|
|17.0     |  0.906     |Почти не связаны|
|7.0      |   0.970    |Почти не связаны|
|8.0      |    0.912   |Почти не связаны|
|13.0     |   0.931    |Почти не связаны|
|11.0     |    0.918   |Почти не связаны|
|16.0     |     0.932  |Почти не связаны|
|14.0     |     0.861  |Mало общего|
|4.0      |      1.003 |Почти не связаны|
|12.0     |     0.999  |Почти не связаны|
|10.0     |      0.754 |Mало общего|
|19.0     |    1.008   |Почти не связаны|
|5.0      |    0.872   |Mало общего|
|15.0     |     0.901  |Почти не связаны|
|18.0     |      0.635 |Умеренное сходство|


 <img width="2618" height="2388" alt="final_dbscan_clusters" src="https://github.com/user-attachments/assets/3471600a-c8c5-4b3e-987b-e53c39cd6b5c" />  

