# Сравнение методов кластеризации на основе семантических эмбеддингов
Этот репозиторий представляет собой практическое исследование в области машинного обучения, посвященное сравнению различных методов кластеризации на задаче семантической группировки биологических терминов.

## Цель:
Сравнить эффективность двух принципиально разных подходов к кластеризации на основе векторных представлений (эмбеддингов) терминов Gene Ontology, полученных с помощью модели Hig2Vec.
Gene Ontology — это стандартизированная система терминов для описания функций генов. Каждый GO-термин имеет определенное семантическое значение. В этом проекте мы используем предобученную модель Hig2Vec, которая преобразует эти термины в векторные представления в многомерном пространстве, сохраняя их семантические связи.

## Задача кластеризации
Автоматически сгруппировать семантически близкие GO-термины в кластеры без предварительных меток, что может помочь в анализе биологических данных, сокращении размерности онтологий и выявлении скрытых паттернов.

## Методы кластеризации
В работе я сравниваю два подхода:
1) Иерархическая кластеризация
2) Понижение размерности + DBSCAN

## Критерии сравнения
1) Качество кластеризации: семантическая однородность кластеров
2) Интерпретируемость: визуальная четкость разделения на группы
3) Практическая применимость: получение разумного количества кластеров 
4) Устойчивость к шуму: способность методов выделять значимые группы

## Исходные данные
1) Все таблицы результатов энричмент-анализа (4 группы: Atx, Young, Old, CS).
2) Скрипт [semantic_model.py](https://github.com/KseniaMartynova/ml/blob/main/semantic_model.py) с функцией: ```get_semantic_matrix(terms, embedding_dict)```.  Он возвращает матрицу размера t × d (t — число терминов, d — размерность эмбеддингов Hig2Vec).
3) Модель Hig2Vec и словарь эмбеддингов.

### Ход работы 
1) Получить с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/join.py) файл [combined_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/combined_terms.csv).
   В нем должны быть столбцы term и description из объединенных таблиц (Atx, Young, Old, CS) по строкам.
2) Собрать уникальные термины из всех групп в файл [all_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/all_terms.csv):  ```all_terms = sorted(set(combined_table["term"]))```
3) Получить эмбеддинги Hig2Vec с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/sorted_embedding.py).  Вызвать функцию: ```X = get_semantic_matrix(all_terms, embedding_dict)```.  
Матрица X нужна для дальнейшей кластеризации.

Шаги с 1 по 3 были общими для обоих методов, далее я буду работать над иерархической клатеризацией.

### Иерархическая кластеризация
1) Нужно построить матрицу семантической близости. Я буду использовать косинусное сходство:  
```from sklearn.metrics.pairwise import cosine_similarity S = cosine_similarity(X) ```
S — матрица t×t, где значения от -1 до 1
[Скрипт](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/s_matrix.py)
3)  [Иерархическая кластеризация](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/clustering.py)
4)  [Подбор оптимального количества кластеров](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/number_cluster_i.py)
5)  [Сформировать итоговую таблицу](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/final_i.py)

### Понижение размерности + DBSCAN
1) Понижение размерности
2) Кластеризация DBSCAN

| Кластер | Расстояние | Оценка |
|---------|------------|--------|
|6.0        0.944        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
3.0        0.948        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
9.0        0.961        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
0.0        0.946        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
1.0        0.970        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
2.0        0.928        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
17.0       0.906        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
-1.0       0.976        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
7.0        0.970        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
8.0        0.912        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
13.0       0.931        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
11.0       0.918        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
16.0       0.932        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
14.0       0.861        ✗ ПЛОХО (мало общего)
4.0        1.003        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
12.0       0.999        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
10.0       0.754        ✗ ПЛОХО (мало общего)
19.0       1.008        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
5.0        0.872        ✗ ПЛОХО (мало общего)
15.0       0.901        ✗ ОЧЕНЬ ПЛОХО (почти не связаны)
18.0       0.635        ⚠ УДОВЛЕТВОРИТЕЛЬНО (умеренное сходство)
