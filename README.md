# Сравнение методов кластеризации на основе семантических эмбеддингов
Этот репозиторий представляет собой практическое исследование в области машинного обучения, посвященное сравнению различных методов кластеризации на задаче семантической группировки биологических терминов.

## Цель:
Сравнить эффективность двух принципиально разных подходов к кластеризации на основе векторных представлений (эмбеддингов) терминов Gene Ontology, полученных с помощью модели Hig2Vec.
Gene Ontology — это стандартизированная система терминов для описания функций генов. Каждый GO-термин имеет определенное семантическое значение. В этом проекте мы используем предобученную модель Hig2Vec, которая преобразует эти термины в векторные представления в многомерном пространстве, сохраняя их семантические связи.

## Задача кластеризации
Автоматически сгруппировать семантически близкие GO-термины в кластеры без предварительных меток, что может помочь в анализе биологических данных, сокращении размерности онтологий и выявлении скрытых паттернов.

## Методы кластеризации
В работе я сравниваю два подхода:
1) Иерархическая кластеризация
2) Понижение размерности + DBSCAN

## Критерии сравнения
1) Качество кластеризации: семантическая однородность кластеров
2) Интерпретируемость: визуальная четкость разделения на группы
3) Практическая применимость: получение разумного количества кластеров 
4) Устойчивость к шуму: способность методов выделять значимые группы

## Исходные данные
1) Все таблицы результатов энричмент-анализа (4 группы: Atx, Young, Old, CS).
2) Скрипт [semantic_model.py](https://github.com/KseniaMartynova/ml/blob/main/semantic_model.py) с функцией: ```get_semantic_matrix(terms, embedding_dict)```.  Он возвращает матрицу размера t × d (t — число терминов, d — размерность эмбеддингов Hig2Vec).
3) Модель Hig2Vec и словарь эмбеддингов.

### Ход работы 
1) Получить с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/join.py) файл [combined_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/combined_terms.csv).
   В нем должны быть столбцы term и description из объединенных таблиц (Atx, Young, Old, CS) по строкам.
2) Собрать уникальные термины из всех групп в файл [all_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/all_terms.csv):  ```all_terms = sorted(set(combined_table["term"]))```
3) Получить эмбеддинги Hig2Vec с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/sorted_embedding.py).  Вызвать функцию: ```X = get_semantic_matrix(all_terms, embedding_dict)```.  
Матрица X нужна для дальнейшей кластеризации.

Шаги с 1 по 3 были общими для обоих методов, далее я буду работать над иерархической клатеризацией.

### Иерархическая кластеризация
1) Нужно построить матрицу семантической близости. Я буду использовать косинусное сходство:  
```from sklearn.metrics.pairwise import cosine_similarity S = cosine_similarity(X) ```
S — матрица t×t, где значения от -1 до 1
[Скрипт](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/s_matrix.py)
3)  [Иерархическая кластеризация](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/clustering.py)
4)  [Подбор оптимального количества кластеров](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/number_cluster_i.py)
5)  [Сформировать итоговую таблицу](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/final_i.py)

### Понижение размерности + DBSCAN
1) Понижение размерности
2) Кластеризация DBSCAN

| Кластер | Расстояние | Оценка |
|---------|------------|--------|
|6.0      |      0.944 |       Почти не связаны|
|3.0      |      0.948 |Почти не связаны|
|9.0      |     0.961  |Почти не связаны|
|0.0      |     0.946  |Почти не связаны|
|1.0      |      0.970 |Почти не связаны|
|2.0      |     0.928  |Почти не связаны|
|17.0     |  0.906     |Почти не связаны|
|7.0      |   0.970    |Почти не связаны|
|8.0      |    0.912   |Почти не связаны|
|13.0     |   0.931    |Почти не связаны|
|11.0     |    0.918   |Почти не связаны|
|16.0     |     0.932  |Почти не связаны|
|14.0     |     0.861  |Mало общего|
|4.0      |      1.003 |Почти не связаны|
|12.0     |     0.999  |Почти не связаны|
|10.0     |      0.754 |Mало общего|
|19.0     |    1.008   |Почти не связаны|
|5.0      |    0.872   |Mало общего|
|15.0     |     0.901  |Почти не связаны|
|18.0     |      0.635 |Умеренное сходство|


| <img width="2618" height="2388" alt="final_dbscan_clusters" src="https://github.com/user-attachments/assets/3471600a-c8c5-4b3e-987b-e53c39cd6b5c" /> | <img width="3180" height="2373" alt="dbscan_parameter_comparison" src="https://github.com/user-attachments/assets/df4797f5-9449-47fd-a9c4-ee66c66c656a" /> |

