# Сравнение методов кластеризации на основе семантических эмбеддингов
Этот репозиторий представляет собой исследование для машинного обучения, посвященное сравнению различных методов кластеризации на задаче семантической группировки биологических терминов.

## Цель:
Сравнить эффективность двух принципиально разных подходов к кластеризации на основе векторных представлений (эмбеддингов) терминов Gene Ontology, полученных с помощью модели Hig2Vec.
Gene Ontology — это стандартизированная система терминов для описания функций генов. Каждый GO-термин имеет определенное семантическое значение. В этом проекте мы используем предобученную модель Hig2Vec, которая преобразует эти термины в векторные представления в многомерном пространстве, сохраняя их семантические связи.

## Задача кластеризации
Автоматически сгруппировать семантически близкие GO-термины в кластеры без предварительных меток, что может помочь в анализе биологических данных, сокращении размерности онтологий и выявлении скрытых паттернов.

## Методы кластеризации
В работе я сравниваю два подхода:
1) Иерархическая кластеризация
2) Понижение размерности + DBSCAN

## Критерии сравнения
1) Качество кластеризации: семантическая однородность кластеров
2) Интерпретируемость: визуальная четкость разделения на группы
3) Практическая применимость: получение разумного количества кластеров 
4) Устойчивость к шуму: способность методов выделять значимые группы

## Исходные данные
1) Все таблицы результатов aнализa обогащения по функциональной принадлежности (4 группы: Atx, Young, Old, CS).
2) Скрипт [semantic_model.py](https://github.com/KseniaMartynova/ml/blob/main/semantic_model.py) с функцией, которая предназначена для преобразования списка GO-терминов в матрицу числовых векторных представлений (эмбеддингов) с использованием предобученной модели Hig2Vec: ```get_semantic_matrix(terms, embedding_dict)```, возвращает матрицу размера t × d (t — число терминов, d — размерность эмбеддингов Hig2Vec). 
3) Модель машинного обучения, разработанная для генерации семантических векторных представлений терминов Gene Ontology - [Hig2Vec](https://github.com/JaesikKim/HiG2Vec?ysclid=mjowu2sbfw299414224) и словарь эмбеддингов (семантическиe векторные представления).
Hig2Vec учитывает: отношения предки + потомки между терминами, часть-of отношения и семантическую близость через общих предков в дереве GO.

### Ход работы 
1) Объединить с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/join.py) все файлы и получить общий - [combined_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/combined_terms.csv).
   В нем должны быть столбцы term и description из объединенных таблиц (Atx, Young, Old, CS) по строкам:
<img width="452" height="217" alt="image" src="https://github.com/user-attachments/assets/746aabee-8fac-4681-aea9-63c97b50e821" /> 

2) Собрать уникальные термины из всех групп в файл [all_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/all_terms.csv):  ```all_terms = sorted(set(combined_table["term"]))```
4) Получить эмбеддинги Hig2Vec с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/sorted_embedding.py).  Вызвать функцию: ```X = get_semantic_matrix(all_terms, embedding_dict)``` из [файла](https://github.com/KseniaMartynova/ml/blob/main/semantic_model.py) 
Получится матрица X, которая нужна для дальнейшей кластеризации.

Шаги с 1 по 3 были общими для обоих методов, далее я буду работать над иерархической клатеризацией и DBSCANом отдельно.

### Иерархическая кластеризация
1) Нужно построить матрицу семантической близости.
Матрица семантической близости используется в биологии для анализа взаимосвязей между биологическими объектами. Такая матрица отражает степень сходства между терминами на основе семантики, закодированной в онтологиях.
Я буду использовать косинусное сходство, которое измеряет косинус угла между двумя векторами:  
```from sklearn.metrics.pairwise import cosine_similarity S = cosine_similarity(X) ```
S — матрица t×t, где значения от -1 до 1
[Скрипт](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/s_matrix.py)
3)  Теперь сама [Иерархическая кластеризация](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/clustering_i.py).
Где D = 1 - S  - это расстояние = 1 - сходство, в результате получается значения расстояний от 0 (максимальное сходство) до 2 (минимальное сходство). Я использую агрегативный(average) подход - каждый термин начинает как отдельный кластер, потом на каждом шаге объединяются два ближайших кластера и процесс объединения продолжается до образования одного кластера. Расстояние между кластерами вычисляется как среднее расстояние между всеми парами элементов: d(u,v) = ∑(d(i,j)) / (|u| * |v|) для всех i в u, j в v
Метод average устойчив к выбросам и создает сбалансированные кластеры. В итоге получается матрица Z, которая соедржит историю слияний.
 4)  [Подбор оптимального количества кластеров](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/hierarchical/number_cluster_i.py). Нам нужно достичь разумного числа кластеров от 20 до 30 и 
семантического сходства примерно ~0.7
5)  [Итоговая таблица](https://github.com/KseniaMartynova/ml/blob/main/datasets/results/hierarchical/final_clusters_with_descriptions.csv). Получается в итоговой таблице: айди термина, номер кластера и описание термина.
#### Итоги иерархической кластеризации
Из [heatmaps](https://github.com/KseniaMartynova/ml/tree/main/datasets/results/hierarchical/heatmap) можно увидеть, что получилось 26 кластеров и что термины в кластерах достаточно близки по смыслу. Однако получилось слишком много кластеров с количеством терминов <=2 и несколько кластеров с непомерно большим количеством терминов. Это все очень нехорошо и достаточно бессмысленно для нашей задачи. Поэтому попробуем еще DBSCAN.

### Понижение размерности + DBSCAN
1) [Понижение размерности](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/dbscan/tsne.py) . - преобразование эмбеддингов Hig2Vec из 200 измерений в 2D пространство, при этом нужно сохранить семантические отношения между терминами. Я буду использовать t-SNE, который минимизирует расхождение между распределениями расстояний в высокоразмерном и низкоразмерном пространствах.
<img width="1279" height="1048" alt="tSNE_simple" src="https://github.com/user-attachments/assets/c1004cdf-831b-4170-94e3-4a668b6a8acd" />
2) [КластеризацияDBSCAN](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/dbscan/clustering.py) . DBSCAN не предполагает сферическую форму кластеров и работает по следующему принципу: сначала идет идентификация ядерных точек, при этом точка считается ядерной, если в радиусе eps находится ≥ min_samples точек; потом ядерные точки, достижимые друг через друга, объединяются в кластер; и остается еще обработать шум: неядерные точки, недостижимые из ядерных, помечаются как шум и заносится в кластер -1.
3) Теперь будем пытаться подобрать оптимальные кластеры: eps будем менять в диапазоне 0.5–2.5, чтобы добиться 15–30 кластеров, и шум не должен быть слишком большим. Я выбираю eps=1.8.
<img width="3180" height="2373" alt="dbscan_parameter_comparison" src="https://github.com/user-attachments/assets/9dc07348-0556-43fc-a297-2ec68e645f06" />
Кластеры для eps=1.8 выглядят хорошо и даже равномерно и создают обманчивое впечатление, получилось 19 кластеров, что кластеризация с помощью DBSKAN лучше для нашей задачи. Чтобы проверить насколько визуальные впечатления не обманчивы сделаем проверку, посчитаем среднее расстояние между всеми терминами из матрицы семантической близости S. Если расстояние близко к 1, то термины в кластере почти не связаны и наша кластеризация в общем то не имеет смысла в рамках данной задачи. Если расстояние близко к 0, то сходство между терминами в кластере очень маленькое и термины в кластере очень семантически схожи. 
#### Итоги DBSCANA
| Кластер | Расстояние | Оценка |
|---------|------------|--------|
|6.0      |      0.944 |       Почти не связаны|
|3.0      |      0.948 |Почти не связаны|
|9.0      |     0.961  |Почти не связаны|
|0.0      |     0.946  |Почти не связаны|
|1.0      |      0.970 |Почти не связаны|
|2.0      |     0.928  |Почти не связаны|
|17.0     |  0.906     |Почти не связаны|
|7.0      |   0.970    |Почти не связаны|
|8.0      |    0.912   |Почти не связаны|
|13.0     |   0.931    |Почти не связаны|
|11.0     |    0.918   |Почти не связаны|
|16.0     |     0.932  |Почти не связаны|
|14.0     |     0.861  |Mало общего|
|4.0      |      1.003 |Почти не связаны|
|12.0     |     0.999  |Почти не связаны|
|10.0     |      0.754 |Mало общего|
|19.0     |    1.008   |Почти не связаны|
|5.0      |    0.872   |Mало общего|
|15.0     |     0.901  |Почти не связаны|
|18.0     |      0.635 |Умеренное сходство|
 
Вот такая таблица получилась у меня, можно сделать вывод, что кластеры красивые да и только, термины в них не связаны по смыслу. Для наглядности можно еще сделать [heatmaps](https://github.com/KseniaMartynova/ml/tree/main/datasets/results/dbscan/heatmaps). Они получаются очень пестрые и по сравнению с иерархической кластеризацей совсем нехорошие.



