# Сравнение методов кластеризации на основе семантических эмбеддингов
Этот репозиторий представляет собой практическое исследование в области машинного обучения, посвященное сравнению различных методов кластеризации на задаче семантической группировки биологических терминов.

## Цель:
Сравнить эффективность двух принципиально разных подходов к кластеризации на основе векторных представлений (эмбеддингов) терминов Gene Ontology, полученных с помощью модели Hig2Vec.
Gene Ontology — это стандартизированная система терминов для описания функций генов. Каждый GO-термин имеет определенное семантическое значение. В этом проекте мы используем предобученную модель Hig2Vec, которая преобразует эти термины в векторные представления в многомерном пространстве, сохраняя их семантические связи.

## Задача кластеризации
Автоматически сгруппировать семантически близкие GO-термины в кластеры без предварительных меток, что может помочь в анализе биологических данных, сокращении размерности онтологий и выявлении скрытых паттернов.

## Методы кластеризации
В работе я сравниваю два подхода:
1) Иерархическая кластеризация
2) Понижение размерности + DBSCAN

## Критерии сравнения
1) Качество кластеризации: семантическая однородность кластеров
2) Интерпретируемость: визуальная четкость разделения на группы
3) Практическая применимость: получение разумного количества кластеров 
4) Устойчивость к шуму: способность методов выделять значимые группы

## Исходные данные
1) Все таблицы результатов энричмент-анализа (4 группы: Atx, Young, Old, CS).
2) Скрипт [semantic_model.py](https://github.com/KseniaMartynova/ml/blob/main/semantic_model.py) с функцией: ```get_semantic_matrix(terms, embedding_dict)```.  Он возвращает матрицу размера t × d (t — число терминов, d — размерность эмбеддингов Hig2Vec).
3) Модель Hig2Vec и словарь эмбеддингов.

### Ход работы 
1) Получить с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/join.py) файл [combined_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/combined_terms.csv).
   В нем должны быть столбцы term и description из объединенных таблиц (Atx, Young, Old, CS) по строкам.
2) Собрать уникальные термины из всех групп в файл [all_terms.csv](https://github.com/KseniaMartynova/ml/blob/main/datasets/all_terms.csv):  ```all_terms = sorted(set(combined_table["term"]))```
3) Получить эмбеддинги Hig2Vec с помощью [скрипта](https://github.com/KseniaMartynova/ml/blob/main/datasets/scripts/sorted_embedding.py).  Вызвать функцию: ```X = get_semantic_matrix(all_terms, embedding_dict)```.  
Матрица X нужна для дальнейшей кластеризации.

Шаги с 1 по 3 были общими для обоих методов, далее я буду работать над иерархической клатеризацией.

### Иерархическая кластеризация
1) Нужно построить матрицу семантической близости. Я буду использовать косинусное сходство:
```from sklearn.metrics.pairwise import cosine_similarity
S = cosine_similarity(X) ```
S — матрица t×t, где значения от -1 до 1
